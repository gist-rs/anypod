Video generation: Sora 2, Kling 2.5 Turbo, and Google’s “Nano Banana” GA

Kling 2.5 Turbo (Text/Image→Video): The latest from Kling tops the Artificial Analysis Video Arena for both text-to-video and image-to-video, edging Hailuo 02 Pro, Google’s Veo 3, and Luma Ray 3. It generates 5s/10s clips up to 1080p. Notable economics: ~$4.20/min on FAL API vs $4.90 for Hailuo 02 Pro and ~$7.32 for Seedance 1.0, and ~15¢ per video on Kling’s Ultra plan via app credits. See model comparisons and pricing in the Arena thread from @ArtificialAnlys and Kling’s announcement @Kling_ai.

OpenAI Sora 2: capability vs. correctness: Live usage shows impressive instruction-following and in-app remixing, but critical evaluations flag physics inconsistencies and marketing polish. See a broad demo roundup @altryne, critiques on “people-pleasing” over physical fidelity @teortaxesTex, and targeted tests where Sora 2 fails physics scenarios that Veo 3 handles better (audio narration correct) @fofrAI, plus a sober overview @Tim_Dettmers.

Google Gemini 2.5 Flash Image (“Nano Banana”) GA: Now production-ready with 10 aspect ratios, multi-image blending, and image-only output. Pricing: $0.039/image on Gemini API (AI Studio + Vertex). Announcements from @sundarpichai, @GoogleAIStudio, and @OfficialLoganK. Also integrated into partner products (e.g., Cartwheel’s new motion pipeline) @andrew_n_carr and showcased by Google’s developer account @googleaidevs.

Ecosystem: Synthesia 3.0 adds “video agents” and new workflows @synthesiaIO.

Open-weight model releases: IBM Granite 4.0 and Qwen updates

IBM Granite 4.0 (Apache 2.0, hybrid Mamba/Transformer): IBM’s new family mixes a minority of standard attention layers with majority Mamba layers to cut memory without large accuracy hits. Sizes include Granite 4.0 H Small (MoE 32B/9B active), H Tiny (7B/1B), H Micro (3B/3B) and a 3B dense Micro variant. Key specs: 128K context, Apache 2.0, strong token efficiency. Artificial Analysis measures H Small at 23 on its Intelligence Index (non-reasoning), ahead of Gemma 3 27B (22) and behind Mistral Small 3.2 (29), EXAONE 4.0 32B (30), and Qwen3 30B A3B (37). Micro scores 16, edging Gemma 3 4B (15). Granite is on HuggingFace and Replicate (H Small at $0.06/$0.25 per 1M in/out tokens). Benchmarks: @ArtificialAnlys. Ollama released runnable images for Micro/Micro-H/Tiny-H/Small-H @ollama. IBM Granite is also added to LM Arena @arena, and HF’s @ClementDelangue highlights browser/WebGPU demos and HF Enterprise onboarding.

Qwen updates: Qwen models are among the first supported by Tinker’s fine-tuning API @wzhao_nlp, and the Qwen team notes expanded support and open releases @Alibaba_Qwen. Qwen-Image-2509 improves consistency @Alibaba_Qwen; Qwen3 VL 235B is reported as performant at lower cost for some vision tasks @scaling01.

Fine‑tuning and systems: Tinker, rank‑1 LoRA, MoE support, and inference speedups

Tinker: a flexible fine-tuning API with LoRA sharing: Thinking Machines’ Tinker lets you write a CPU-only training loop and run it unchanged on distributed GPUs, keeping control over algorithms/losses while Tinker manages scheduling, resource allocation, and failures. It supports open models (Llama, Qwen) including large MoE (e.g., Qwen3-235B), and implements LoRA for efficient resource sharing. Summaries: @TheTuringPost, release note @Smol_AI, cookbook/docs: link.

LoRA without regrets (rank=1): Multiple replications show rank-1 LoRA can match full fine-tuning quality on reasoning tasks while saving ~43% VRAM, enabling RL on larger models; see results and code @zzlccc and a Colab on Qwen3-0.6B OpenR1-Math @ben_burtenshaw. See guidance from “LoRA Without Regret” @TheTuringPost.

MoE training and infra: Prime-RL now supports MoE for RL and SFT (Qwen3 A3-30B, GLM series, Moonlight), with significant modeling rewrites to stay Torch Compile compatible while retaining HF ecosystem compatibility @samsja19. On inference, @vikhyatk reports a new engine with 1.3–20x faster completions; production uses QAT for FP8 KV caches and MoE weights (engine proprietary for now). For local/dev infra: MI300X VMs on-demand at $1.99/GPU/hr @HotAisle, vLLM now supports BERT @vllm_project.

RL and reasoning: search‑in‑training, broadened exploration, latent CoT, front‑loaded reasoning

Train-time search and efficient exploration: DeepSearch moves MCTS into the training loop with Tree‑GRPO stabilization and efficient caching/filtering, reaching 62.95% on AIME/AMC with ~330 GPU hours (beating a Nemotron baseline and outpacing standard RL that plateaus even with 1800+ GPU hours) @omarsar0. BroRL scales exploration by increasing rollouts per example into the hundreds, overcoming the saturation seen when only scaling training steps @iScienceLuvr.

Architectures and training mechanics: A new latent CoT method “thoughtbubbles” inserts input‑adaptive latent tokens to allocate more compute without CoT labels, improving perplexity and compute use @houjun_liu with positive reaction @khoomeik. NVIDIA’s “Front‑Loading Reasoning” finds injecting reasoning during pretraining yields durable gains that finetuning can’t recover @__SyedaAkter. A small but impactful MoE tweak—global‑batch load balancing (vs micro-batch) —yields lower perplexity and clearer expert specialization with minimal code changes @daddyofadoggy. For sparse diffusion LMs, OpenMoE 2 studies expert‑choice MoE × diffusion across wide FLOPs/param regimes, claiming perfect load balance (no aux loss), +20% throughput, and adaptive compute under multi‑epoch training @NiJinjie.

Agents and toolchains: CLI + semantic search, Notebook MCP, browsers, and CLIs

CLI agents + semantic search beat pure CLI: LlamaIndex’s SemTools benchmark (1,000 arXiv papers) shows agents with semantic search produce more complete answers across question types versus agents using only CLI tools; Unix tools remain a strong baseline and SemTools integrates parse (LlamaParse) and semantic search directly into command-line agents (Claude/Gemini CLIs). Results/methodology: @llama_index.

Executing notebooks via MCP: Goodfire open-sourced Scribe, an MCP-based system enabling agents to run notebook cells and receive Jupyter outputs (text/errors/images). They share lessons on “experimenter agents” vs “software development agents” and the scaffolding needed for scientific workflows @GoodfireAI, blog.

“AI browsers” and evaluators: Perplexity’s Comet is now GA globally, with Comet Plus launching alongside major publisher partnerships; Pro/Max users get Plus bundled @perplexity_ai, @AravSrinivas. Yupp’s “Help Me Choose” orchestrates a third model to critique two candidate answers, then has them analyze each other before the user picks — an interesting pattern for adjudication @yupp_ai, @lintool. Google’s Jules Tools brings an agentic CLI (npm installable) mirroring browser capabilities @julesagent.

Leaderboards and real‑world coding agent metrics

Claude Sonnet 4.5 tied for #1 on LM Arena: Sonnet 4.5 reaches the top slot alongside Claude Opus 4.1, with strong showings across categories including coding and creative writing (rankings are from tens of thousands of human votes) @arena. Community reports suggest Anthropic continues to ship very competitive coding models @scaling01.

Open source is closing in for code editing agents: In Cline’s diff‑edit success tests, GLM‑4.6 achieves 94.9% vs Claude 4.5’s 96.2% at ~10% of the cost; users report switching workflows accordingly @cline, @nickbaumann_.

Video Arena reminder: Kling 2.5 Turbo leads both T2V and I2V; details above in the Video section @ArtificialAnlys.


Anthropic’s Claude Sonnet 4.5: capabilities, coding, and early evals

Claude 4.5 Sonnet (200K ctx, 64K max output): Anthropic’s upgrade brings higher intelligence at the same price as Sonnet 4 ($3/$15 per 1M input/output), with improved token efficiency even in “Thinking” mode. Independent evals from Artificial Analysis place it behind GPT‑5-high but ahead of Gemini 2.5 Pro and Grok 4 Fast, while remaining notably frugal with output tokens; they also note larger gains in agentic tool use and safety/alignment behaviors than in prior benchmarks (thread). On ARC‑AGI, Sonnet 4.5 tracks GPT‑5 closely with performance scaling meaningfully at higher thinking budgets (@GregKamradt; commentary). Users report standout “state management” and context compaction, making long agentic workflows more reliable (@nickbaumann_; @skirano). Ecosystem support landed quickly: LangSmith cost tracking/playground (@Hacubu), ARC Prize results (@scaling01), and community measurements on LiveBench and Deep Research Bench with strong coding/math placements (1, 2).

Claude Code 2 and agent stack: Anthropic shipped Claude Code v2, VS Code extension updates, context editing and memory tools (launch roundup). Replit reports Sonnet 4.5 improves reliable code edits and autonomy in Agent 3 (@pirroh). Anthropic also published an engineering blog on “context engineering” (beyond prompt engineering) for agent systems (@AnthropicAI).

Zhipu’s GLM‑4.6 (open weights) and agentic coding focus

GLM‑4.6 release (MIT license): Zhipu extends the GLM‑4.5 line with 200K context, stronger coding, improved reasoning/tool use, and better agent task success, while using ~15% fewer tokens per trajectory vs 4.5. Zhipu published CC‑Bench‑V1.1 (74 real-world agentic coding tasks with full trajectories) showing GLM‑4.6 near-parity to Claude Sonnet 4 in coding and leading domestic peers, with all eval details open (@Zai_org, bench; analysis by @gm8xx8). Open weights and API are live; hosting on HF/ModelScope incoming.

Ecosystem uptake: Available on OpenRouter (@OpenRouterAI), Yupp (@yupp_ai), YouWare (@YouWareAI), Roo Code (@roo_code), Cline (@cline), and Anycoder (@_akhaliq). Locally, MLX runs GLM‑4.6 at ~17 tok/s on M3 Ultra (5.5 bpw quant; 5.3K tokens) (@awnihannun).

Frontier video models: Sora 2 launch and early comparisons

OpenAI Sora 2 and app: OpenAI released Sora 2 with an iOS app (US/Canada invite-only at launch), cameo features (consent controls, watermarks), and a system card; Android and API are planned. OpenAI emphasizes “world simulation” demos with improved physics/steerability and audio, while acknowledging the risks of algorithmic feeds and deepfakes (product post, teaser, Sam Altman’s note). Reactions are mixed: some highlight standout realism/consistency; others point to artifacts and note Google’s Veo 3 as competitive in certain cases (pro, skeptic, physics demo).

Luma Ray 3: Luma’s new Ray 3 ranks #2 in Artificial Analysis’ T2V Video Arena, introducing an iterative chain-of-thought generation loop and 16-bit HDR support (I2V/T2V up to 10s 1080p). API not yet available (@ArtificialAnlys).

Training efficiency and post-training: FP4, QAT, and RL during pretraining

NVFP4 (NVIDIA): 4‑bit pretraining with 2‑level scaling, RHT, and stochastic rounding matches FP8 baselines on a 12B model trained on 10T tokens (MMLU‑Pro 62.58 vs 62.62), promising ~6.8× efficiency and ~50% lower memory; Blackwell supports FP4 matmul and required rounding modes (paper/code, summary). Open-source TE support is in progress.

Compute-Optimal QAT (Apple): A scaling law for budgeting quantization-aware training vs full-precision given tokens/memory; practical guidance for planning QAT as a first-class citizen in training schedules (@aldrmv, @awnihannun).

RLP (NVIDIA): Reinforcement Learning Pre‑training teaches models to “think before predicting” with a verifier‑free, dense information-gain reward on web text, yielding sizable boosts over base models (e.g., +19% Qwen3‑1.7B, +35% Nemotron‑Nano‑12B on math/science suites) and compounding with post‑training (paper/blog).

Learning from users and agent memory

RLHI (Meta): Reinforcement Learning from Human Interaction trains directly from organic user conversations (user-guided rewrites and user-based rewards), outperforming baselines on personalization and instruction following while retaining standard benchmark performance (@jaseweston, paper).

ReasoningBank (agents): A memory system that stores distilled strategies from both successes and failures to improve reuse and efficiency in web/SWE tasks, reporting +34.2% efficiency and –16% steps vs prior memory methods (tweet).

Efficient sequence models: SWAX combines sliding-window attention with xLSTM and stochastic window sizes to boost both short/long recall (tweet). For diffusion LMs, SparseD proposes sparse attention (1.3–1.5× faster near‑lossless) and LLaDA‑MoE (sparse MoE dLLM) reports SOTA among diffusion LLMs with smaller active params (SparseD, LLaDA‑MoE). Finally, MobileLLM‑R1 shows sub‑billion parameter reasoning models (950M) hitting AIME 15.5 with ~2T tokens of curated data and standard post‑training (tweet).

Agentic coding stacks and infra

Local and hosted agent stacks: AMD endorsed local “vibe coding” with Cline + LM Studio, recommending Qwen3‑Coder‑30B (4/8‑bit) and GLM‑4.5‑Air for higher RAM tiers (@cline). AI SDK now routes to any HF model (@nishimiya). Cursor 1.7 adds prompt suggestions and org-wide rules (@cursor_ai). Sim launched a fully local, open-source drag‑and‑drop agentic workflow builder with MCP integrations (thread).

Codex vs Claude Code operational choices: Reverse‑engineering notes emphasize OpenAI Codex CLI’s shell‑first loop (think→tool→observe), unified diffs to reduce error surface, and OS‑level sandboxing vs heavier tool orchestration (analysis). Meanwhile, GitHub MCP Registry and Claude extensions continue to mature in VS Code (@code, @gallabytes).

Periodic Labs: AI scientists + autonomous labs

Led by Liam Fedus and Doğuş Ekin, Periodic raised a $300M founding round led by a16z to build AI scientists paired with autonomous labs for verifiable, experiment‑driven science—targeting materials (e.g., superconductors) and semiconductor advances; team includes alumni behind ChatGPT, GNoME, attention, MatterGen, and scaled autonomous physics labs (launch, a16z). The thesis: internet text is finite; progress needs new, high‑signal experimental data and closed‑loop verification.
