OpenAI's Evals team is back for a third time this year with GDPVal, which they are framing as a logical next step in model evals with the breadth of MMLU, but with the depth of agentic benchmarks like SWE-Bench and their own SWE-Lancer. GDPval (full paper here) takes its name from a top down selection of major (>5%) sectors of GDP, filtered for "predominantly digital" knowledge work:




This resulted in 1,320 tasks across 44 occupations, which were then evaluated against models and human experts averaging 14 years of experience in those fields:






The two primary results charts are hugely validating: first that OpenAI doesn't bias towards itseslf, and that Opus is within spitting distance of industry expert output:




and the model trendlines over time have GPTnext matching human performance roughly by mid 2026:




The word AGI isn't mentioned at all in the paper, but the original 2018? OpenAI charter defined AGI as "highly autonomous systems that outperform humans at most economically valuable work”. If we were to wake up in Sept 2026 and find that GPT6-high-ultrathink-final-for-realsies was above confidence interval of 50% in GDPVal pairwise comparisons, then we could truly say that we have achieved AGI by 2018 standards.



AI Twitter Recap
OpenAI’s GDPval and the state of real‑world evals

GDPval (OpenAI): OpenAI introduced GDPval, a new eval measuring model performance on “economically valuable” tasks across 44 occupations, with tool use (search/code/doc) and multi-hour complexity. Early results: Claude 4.1 Opus tops most categories, approaching or beating human industry experts; GPT‑5 “high” trails Opus on the same tasks. OpenAI provides a public site and methodology; leadership frames this as a key metric for policymakers and forecasting labor impact. See launch and discussion: @OpenAI, @kevinweil, @gdb, @dejavucoder, @Yuchenj_UW, @LHSummers.

Artificial Analysis indices:

Gemini 2.5 Flash/Flash‑Lite (Preview 09‑2025): +3/8 points (reasoning/non‑reasoning) for Flash; +8/+12 for Flash‑Lite vs previous releases. Flash‑Lite is ~40% faster (≈887 tok/s) and uses 50% fewer output tokens; 1M context, tool use, and hybrid reasoning modes. Pricing: Flash‑Lite $0.1/$0.4 per 1M in/out; Flash $0.3/$2.5. Benchmarks: @ArtificialAnlys, follow‑up.

DeepSeek V3.1 Terminus: +4 points over V3.1 (reasoning mode), large gains in instruction following (+15 IFBench) and long context (+12 AA‑LCR). Architecture: 671B total, 37B active; availability via API and third‑party hosts (FP4/FP8). @ArtificialAnlys.

AA‑WER (speech‑to‑text): New word‑error‑rate benchmark across AMI‑SDM, Earnings‑22, VoxPopuli. Leaders: Google Chirp 2 (11.6% WER), NVIDIA Canary Qwen2.5B (13.2%), Parakeet TDT 0.6B V2 (13.7%). Price/perf tradeoffs noted; Whisper/GPT‑4o Transcribe smooths at cost to literal accuracy. @ArtificialAnlys, pricing.

Agentic coding and productized agents

Kimi “OK Computer” (K2‑powered agent mode): An OS‑like agent with its own file system, browser, terminal and longer tool budgets. Demos: single‑prompt websites/mobile‑first designs, editable slides, and dashboards from up to 1M rows. Also released a Vendor Verifier for tool‑call correctness by provider on OpenRouter. Threads: @Kimi_Moonshot, @crystalsssup, examples 1, 2.

GitHub Copilot CLI (public preview): Local terminal agent with MCP support that mirrors the cloud Copilot coding agent. Use existing GitHub identity, script embedding, clear per‑request billing. Announcements: @github, @lukehoban.

Factory AI “Droids” + $50M: Model‑agnostic software dev agents (CLI/IDE/Slack/Linear/Browser), #1 on Terminal‑Bench, pitched as broader knowledge‑work agents via code abstractions. Launch + funding: @FactoryAI, commentary @swyx, @tbpn.

Ollama web search API + MCP server: Bridges local/cloud models to live web grounding; compatible with Codex/cline/Goose and other MCP clients. @ollama.

Reka Research “Parallel Thinking”: API option that generates multiple candidate chains and resolves via a verifier model; +4.2 on Research‑Eval and +3.5 on SimpleQA with near‑flat latency. @RekaAILabs.

Video reasoning and robotics

Video models as zero‑shot reasoners (Veo 3): DeepMind shows broad zero‑shot skills across perception → physics → manipulation → reasoning. Introduces “Chain‑of‑Frames” as visual CoT. Still behind SOTA on depth/physics; cost remains high. Papers/discussion: @arankomatsuzaki, project/paper, @tkipf.

Gemini Robotics 1.5 (Google): New embodied reasoning stack (GR 1.5 VLA + ER), long context, tool use, spatial‑temporal planning, transfer across embodiments, and safety constraints. API in Google AI Studio; sorting‑laundry reasoning demo. Announcements: @GoogleDeepMind, @sundarpichai, API note, @demishassabis.

Model and method releases

EmbeddingGemma (Google): A 308M encoder model topping MTEB among sub‑500M models (multilingual/English/code). Claims parity with ~2× larger baselines; supports 4‑bit and 128‑dim embeddings. Techniques: encoder‑decoder init, geometric distillation, spread‑out regularizer, model souping. Good for on‑device/high‑throughput. Threads: @arankomatsuzaki, paper roundup.

ShinkaEvolve (Sakana AI, open source): A sample‑efficient evolutionary framework that “evolves programs” using LLM ensembles with adaptive parent sampling & novelty filtering. Results: new SOTA circle packing with 150 samples; improved ALE‑Bench solutions; discovered a novel MoE load‑balancing loss improving specialization/perplexity; stronger AIME scaffolds. Code/paper: @SakanaAILabs, @hardmaru, report.

RLMT & TPT:

“Language Models that Think, Chat Better” proposes RL with Model‑rewarded Thinking (RLMT) to surpass RLHF on chat benchmarks for 8B models; ablations emphasize prompt mixtures and reward strength. @iScienceLuvr, notes.

“Thinking‑Augmented Pre‑Training (TPT)” reports ~3× pretrain data efficiency and >10% post‑training improvements on reasoning for 3B models via synthetic step‑by‑step trajectories. @iScienceLuvr.

Systems, serving, and infra

Perplexity Search API: A real‑time web index with state‑of‑the‑art latency/quality for grounding LLMs and agents, plus public evals/research. Claims strong performance vs single‑step and deep research benchmarks, and advantages vs Google SERP for LLM use. Launch: @perplexity_ai, research: article, commentary: @AravSrinivas.

KV reuse and dynamic parallelism:

LMCache: Open KV‑cache layer that reuses any repeated text segment (not just prefixes) across GPU/CPU/disk; reduces RAG cost 4–10×, TTFT, and boosts throughput. Integrated in NVIDIA Dynamo. @TheTuringPost.

Shift Parallelism (Snowflake): Dynamically switches Tensor/Sequence Parallelism based on load—up to 1.5× lower latency (interactive) and 50% higher throughput (heavy traffic). Code in Arctic Inference. @StasBekman.

Context‑parallel diffusion: Native support for ring/Ulysses variants to make multi‑GPU diffusers “go brrr.” @RisingSayak.

attnd (ZML): Sparse logarithmic attention on CPU, over UDP; pitched as “paving the way for unlimited context.” @steeve.

Energy and hardware:

Microsoft (LLM inference energy): Median chatbot query ~0.34 Wh; long reasoning ~4.3 Wh (~13×); fleet at 1B q/day ~0.9 GWh (~web search scale). Claims public estimates are 4–20× too high; 8–20× efficiency gains feasible. @arankomatsuzaki.

B200 spot pricing: B200 spot instances briefly at ~$0.92/hr. @johannes_hage.

Industry moves and platform updates

Meta talent coup: Diffusion/consistency models pioneer Yang Song departs OpenAI to join Meta; widely regarded as a major poach. Coverage: @iScienceLuvr, @Yuchenj_UW.

ChatGPT Pulse: OpenAI rolls out “proactive” daily updates (context, connected apps) to Pro users—an ambient agent form factor moving beyond reactive chat. Threads: @OpenAI, @sama, @fidjissimo.

Qwen ecosystem: Qwen models added to the LMSYS Arena (@Alibaba_Qwen); Qwen3‑VL provisioning via third‑party providers for easier trials. @mervenoyann.
