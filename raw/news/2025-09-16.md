OpenAI’s GPT-5-Codex and the agentic coding race

OpenAI ships GPT-5-Codex (agentic coding): OpenAI released a GPT-5 variant optimized for long-running, tool-using software engineering across the Codex CLI, IDE extension, web, GitHub code reviews, and ChatGPT iOS. Highlights: dynamic “task-adaptive” thinking (15x faster on easy tasks, 2x more deliberate on hard ones), multi-hour autonomy (“>7 hours” on complex tasks), improved instruction-following and code quality, and better SWE-bench–style performance. OpenAI also referenced an unreleased large “refactor” benchmark where GPT-5-Codex reaches 51% accuracy and indicated SWE-bench fixes for apples-to-apples comparisons. See announcements and discussion from @OpenAI, @gdb, @sama, @OpenAIDevs, @OfirPress, @swyx and routing/depth behavior notes (“router in the model”) by @swyx. Early hands-on reports range from “more steerable and persistent” (@omarsar0) to frustration over token burn and long loops (#1, #2). OpenAI also teased deep OS integrations (e.g., Xcode sign-in for GPT‑5) via @OpenAIDevs.

Evals and coding depth: OpenAI claims SWE-bench improvements and a new internal “large refactor PR” eval; community called for public versions (@OfirPress). There’s broad agreement that variable compute and routing are critical to efficiency and quality at inference time (@swyx; @polynoamial).

Qwen3‑Next 80B (A3B MoE), long-context, and the China efficiency push

Qwen3‑Next‑80B (3B active) lands on Together + NVIDIA NIM: Alibaba’s hybrid MoE model targets long-context (native 262k, extensible to 1M+), repository-scale code analysis, and efficient reasoning. Together AI provides “Instruct” and “Thinking” endpoints (launch, contexts), and NVIDIA added NIM support with CUDA-accelerated attention (NVIDIA). Alibaba reports strong performance “with only 3B active parameters” (@Alibaba_Qwen) and head-to-head results vs Gemini 2.5 Flash Thinking on reasoning benchmarks (@togethercompute). On-device MLX numbers show eye-catching TPS on Apple hardware (@ivanfioravanti, batching).

Architecture trend: hybrid SSM + MoE: In the past two weeks, 6 of 7 new MLX-LM architectures are MoE, half hybridizing SSM/attention (@awnihannun, list). Context from China v. US training regimes: constrained flops driving infra/model co-design, token efficiency, linear attention, and test-time scaling focus (@JingyuanLiu123). Community sentiment echoes that small models are increasingly capable, given the right recipes (@Thom_Wolf).

Tooling for agents: MCP everywhere, Claude Code SDK, and workflow “vibe coding”

MCP consolidation: The Model Context Protocol’s value-prop—turn M×N tool integrations into M+N via MCP servers—continues to resonate (diagram). New OSS appears across the stack: DeepMCPAgent (LangChain/LangGraph-based MCP agents) (repo), Markdown MCP (@dariusemrani), and enterprise hackathon showcases (thread). LangChain shipped reactive agent examples (news curation, ParserGPT, human-in-the-loop for Deep Agents) (news agent, parser, HITL).

Claude Code SDK adds agent ergonomics: Anthropic shipped code references, custom tools, and hooks support, making bespoke agents faster to build (@_catwu). Replit’s Agent 3 (no-code “vibe” workflows) and Poke (iMessage agents orchestrating ephemeral subagents) show the “agent UX” frontier moving quickly (Replit demo, Poke deep dive).

RL for reasoning and agents: online RL in product, deep research agents, and new training regimes

Online RL in production assistants: Cursor’s rollout is widely cited as a first at scale for frontier capability, with enthusiasm around moving continuous training cycles from months → weeks → hours (@willdepue, follow-up). Strong interest persists in post‑GRPO advances (@vikhyatk).

Deep research agents (single-agent RL > complex scaffolds): A new study shows a simple RL recipe with length-normalized rewards and strategic tool limits can train single agents that rival multi-agent setups; test-time scaling also helps (parallel searches + pick the shortest successful trajectory) (summary, paper).

HRL and decentralized RL: Meta’s Scalable Option Learning re-architects hierarchical RL for GPU-parallel batch updates (25× training speedups) (explainer). Gensyn’s SAPO shares rollouts in plaintext across a “swarm” of heterogeneous nodes (up to +94% cumulative reward) (@TheTuringPost). Tencent’s SimpleVLA-RL scales VLA training via RL (paper).

Long-horizon execution: Multiple analyses argue small step-accuracy gains compound exponentially in long chains; many failures are execution (not reasoning) errors; “thinking” models reduce harmful self-conditioning (@HuggingPapers, @TheTuringPost, @emollick).

Multimodal and computer-use models

Holo1.5 for computer-use agents (open weights): H’s new VLMs (3B, 7B Apache-2.0, 72B) set SOTA on UI localization and QA—core skills for reliable web/mobile use. Open weights, cookbook, and demos are available (launch, H company, cookbook).

Tencent SRPO (diffusion RL for aesthetics/realism): “Self-Regulating Preference Optimization” fine-tunes FLUX1dev along the full denoising trajectory, boosting human-rated realism/aesthetics >3×; code and Space are live and trending (overview, demo).

MobileLLM-R1 (Meta) and on-device reasoning: Meta introduced small-from-scratch reasoning models (0.14B/0.35B/0.95B; ~4.2T pretraining tokens) with a 140M variant running fully in-browser (announce, demo).

New datasets/benchmarks: SpatialVID (7k+ hours with dense 3D annotations) for spatial video intelligence (@HuggingPapers), and IntrEx (sequence-level interestingness labels in educational dialogues) (@HuggingPapers).

Systems and infra (throughput, routing, and deployment)

Throughput milestones and platform support: Fireworks reported 540 tokens/s on GPT‑OSS‑120B running on B200, exceeding a leading ASIC in their test (@lqiao). vLLM 0.10.2 adds aarch64 support (install vLLM directly on GB200; multi-platform images) with more perf on the way (@vllm_project). Ray 2.49 introduced prefix cache–affinity routing to maintain KV-cache hit rates across large vLLM fleets (@seiji_________).

Batching and fleets: Together released a revamped Batch Inference API (unified UI, all models, 3,000× higher rate limits—30B tokens—and 50% discounts for most serverless models) (launch). Prime Intellect opened Reserved Instances for 8–1,000+ GPU clusters with secondary resale to spot markets (announce).

Kernel and Apple-side speedups: Standard Kernel previewed minimal CUDA+PTX kernels surpassing cuBLAS/FlashAttention3 on targeted ops; fused LLaMA3 FFN claimed 120% PyTorch perf (@anneouyang). MLX continues to mature with high-TPS batching on M3 Ultra and shorter full-suite eval times (TPS, MMLU-Pro runtime).

Qwen as deployable building block: NVIDIA added Qwen3‑Next NIMs; Baseten and Together integrated the “Thinking”/“Instruct” variants for production use (NVIDIA, Baseten, Together).
