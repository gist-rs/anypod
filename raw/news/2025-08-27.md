Gemini 2.5 Flash Image (“nano-banana”) dominates image editing

Model reveal, capabilities, availability: The anonymous “nano-banana” on community arenas was confirmed as Gemini‑2.5‑Flash‑Image‑Preview by Google DeepMind. It delivers state-of-the-art image editing and generation with standout strengths in character consistency, targeted natural-language edits, multi-image composition, and accurate text rendering. It’s live in the Gemini app, Google AI Studio/API, and surfaced early across eval sites (@GoogleDeepMind, @sundarpichai, @Google, docs, pricing).

Benchmarks and usage at scale: On the Image Edit Arena, Gemini 2.5 Flash Image leads by an unprecedented ~170–180 Elo against the next best, with >5M votes across two weeks and >2.5M votes on this model alone—the largest margin in Arena history. It now ranks #1 for image editing and #1 or top-tier for text-to-image in community leaderboards (@lmarena_ai, reveal, usage spike, Artificial Analysis). Cost is cited as $30 per 1M output tokens (about 1,290 tokens per image, i.e., ~$0.039/image) (@_philschmid, @andrew_n_carr). Multiple demos highlight multi-turn conversational editing, consistent persona re-rendering, and implicit “world knowledge” in visual edits (@skirano, @omarsar0).

Ecosystem availability: The model is already integrated on third-party platforms and leaderboards (e.g., Yupp, LMArena battle mode, OpenRouter as a launch partner), with community prompting guides rolling out (@yupp_ai, @xanderatallah, @OfficialLoganK).

New models and open-source releases

Nous Research Hermes 4 (open weights): Hybrid “reasoning” models focused on steerability, low refusals, and strong math/coding/STEM benchmarks. Available on Hugging Face and OpenRouter, with “thinking” mode toggles via headers/template kwargs (@NousResearch, weights, OpenRouter, toggle).

NVIDIA Nemotron Nano 9B V2 (reasoning small model): A hybrid Mamba‑Transformer, 128k context model trained by NVIDIA (not Llama-derived), released under the NVIDIA Open Model License (no Llama restrictions). Supports reasoning/non-reasoning modes (system “/no_think”), reported as top-performing <10B model on one leaderboard; NVIDIA also released a 6.6T-token pretraining subset on Hugging Face (@dl_weekly, @ArtificialAnlys, NVIDIA blog).

InternVL3.5 (VLMs): First VLMs built off OpenAI’s gpt‑oss line are out, with a diverse set of 32 models spanning pretraining, finetuning, and alignment, using either gpt‑oss or Qwen3 as the LLM backbone (@mervenoyann).

Ollama v0.11.7: Adds DeepSeek v3.1 support (hybrid “thinking”) across the app/CLI/API/SDKs with Turbo mode in preview (@ollama).

Apple Silicon local stack: “Osaurus” is a lightweight (~7MB) MLX-based Apple Silicon-native LLM server claiming ~20% faster than Ollama; community is porting multiple small models to MLX (@geekbb, @LiMzba).

Also of note: Liquid AI’s LFM2‑VL series (@dl_weekly) and a strong French finetune of LFM2 by students using FFT+merging (@maximelabonne).

Agents, APIs, and developer tooling

Claude for Chrome (research preview): Anthropic is piloting a browser-integrated actioning agent for 1,000 users. Emphasis is on safety—especially prompt injection defenses—prior to broader rollout (@AnthropicAI, safety note).

OpenAI API changes: Assistants API is officially deprecated in favor of the Responses API (sunsets Aug 26, 2026). Responses now carries code interpreter, persistent conversations, MCP, and computer use; with GPT‑5, “reasoning tokens” are preserved between turns. Web search in Responses gets domain filtering, source reporting, and a price cut to $10/1K calls (from $25) (@OpenAIDevs, pricing update).

Agent architecture and evaluation: Cline argues many 2023 patterns—multi-agent orchestration, codebase-indexed RAG, and instruction overloading—often underperform vs simpler designs today (thread, blog). TransluceAI’s Docent alpha automates behavior analysis at scale (reward hacking, instruction violations), with early testers from major labs and evaluations orgs (launch). Weave+Tavily released recipes for traceable, current research agents (Weave). LangGraph Studio’s update improves interactive debugging and tracing UX (@LangChainAI). Weaviate’s Elysia offers an “agentic RAG” UI with dynamic displays beyond text (@weaviate_io). Beam ships an OSS “decorator-to-serverless” framework (@_avichawla).

Training, RL, and optimization

GRPO demystified with code: Clear walkthroughs of GRPO applied to train Qwen 2.5 to play 2048, with runnable code and an explainer video (@jayendra_ram). Community quips that “RL with LLMs is just massaging your KV cache to fit in memory” capture the practitioner reality (@finbarrtimbers).

RL frameworks snapshot: A roundup contrasts verl (Ray/DataProto infra; SGLang integrated; scaled to 671B), AReal (Ant’s async RL), Nemo‑RL (NVIDIA; strong perf, later to adoption), and Zhipu’s Slime (SGLang/Megatron-optimized). On‑policy is cleaner, but off‑policy often wins in practice due to rollout/inference bottlenecks in post‑training (summary).

Long-context and compression: Hugging Face Trainer now supports context parallelism for 100k+ sequence lengths (@m_sirovatka). vLLM’s LLM Compressor v0.7.0 adds transform support (QuIP, SpinQuant), mixed precision, better MoE handling (Llama‑4), and NVFP4/FP8 mixes (@vllm_project). Research/threads cover Adam’s scale invariance caveats via epsilon tuning (@sedielem) and adaptive batching for comms efficiency (AdLoCo) (@papers_anon).

Data pipelines are evolving: Trend from “more data with light filters” to aggressive LLM-based filtering + replay for longer training (FineWeb‑edu/HQ, DCLM), and now LLM rephrasing to extract more signal per sample (e.g., Nemotron‑CC, WRAP, REWIRE). Multi‑epoch training is back in favor with diminishing returns accepted (@lvwerra).

Systems and infra notes

Google TPUv7 architecture (Hot Chips): First public block diagram for TPUv7 (aka v6p/“ghostfish”): 8 stacks of HBM3e, 4 medium-size systolic arrays, 3D torus scale-up to 9,216 devices. OCS reduces but does not eliminate failure-domain “blast radius” in 3D torus topologies (@SemiAnalysis_).

Platforms: zml/llmd now runs on TPU with full prefill/decode paged attention and a single flag (@steeve); Hugging Face Diffusers deprecates Flax to go PyTorch-first (@RisingSayak). Slurm support landed for H100/H200/B200 multi-node setups on Prime clusters (@jannik_stra).

Benchmarks and reasoning research

Reasoning/math: IneqMath adds judges, more data, local vLLM support, and a continually updated leaderboard; SOTA results now at 47% overall with GPT‑5 (medium, 30K) vs 23.5% for the best open model (gpt‑oss‑120B, 10K) (@lupantech). Stanford’s UQ benchmark probes whether LLMs can solve curated unsolved problems across domains; some model solutions passed expert validation (@Muennighoff). MIRAGE explores graph‑retrieval augmented multi-chain reasoning with interpretable KG chains and budget tuning (@omarsar0). A new interpretability result links neuron feature “overpacking” to adversarial fragility (@GoodfireAI). And a historical note: “scaling laws” discussions predate 2017/2020 work—see NIPS 1993’s learning curves and test error prediction (@jxmnop).

Top tweets (by engagement)

Gemini 2.5 Flash Image (banana trifecta): Announcements and demos from Sundar Pichai and Google DeepMind drove massive engagement (@sundarpichai, @GoogleDeepMind, @googleaistudio).

Anthropic’s agentic push: Claude for Chrome research preview, focused on safe browser actioning (@AnthropicAI).

Community validation: Demis Hassabis calling Gemini 2.5 Image the best available model by a wide Elo margin (@demishassabis); Oriol Vinyals on usage and Arena virality (@OriolVinyalsML).

Reid Hoffman’s heuristic: “10,000 prompts is the new 10,000 hours” captured the zeitgeist of practice-driven mastery (@reidhoffman).

Scale AI x US Army: Industry momentum continues with a $99M U.S. Army contract announcement (@alexandr_wang).
