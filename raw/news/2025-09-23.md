OpenAI × NVIDIA: 10 GW and “millions of GPUs.” OpenAI announced a strategic partnership with NVIDIA to deploy at least 10 gigawatts of GPU datacenters, targeting first capacity in 2H 2026 on “Vera Rubin,” with NVIDIA intending to invest up to $100B as systems are deployed. OpenAI framed NVIDIA as a preferred strategic compute/networking partner; NVIDIA’s market cap jumped on the news. Details via @OpenAINewsroom and @gdb. Commentary on how such scaling continues to drive down the “cost of intelligence” from @ArtificialAnlys.

Deterministic inference for RL & reproducibility: SGLang added end‑to‑end deterministic attention/sampling that remains compatible with chunked prefill, CUDA graphs, radix cache, and non‑greedy sampling—useful for reproducible rollouts and on‑policy RL with minimal overhead. See @lmsysorg.

FP8, comms, and real‑world speedups: Practitioners reported tangible FP8 gains under parallelism with comms constraints (e.g., PCIe), with perf crossover vs BF16 under pipeline/data parallel regimes. See local results and methodology from @TheZachMueller and follow‑ups. Related: Together AI is offering early access to GB300 NVL72 racks (@togethercompute).

Write once, run on many GPUs: Modular previewed cross‑vendor portability where most code written for NVIDIA/AMD “mostly just works” on Apple Silicon GPUs—aimed at lowering hardware access barriers (@clattner_llvm). See also their updated cross‑vendor stack notes (@clattner_llvm).

Major model drops: Qwen3 Omni family, Grok‑4 Fast, DeepSeek V3.1 Terminus, Apple Manzano, Meituan LongCat

Qwen’s multi‑front release wave:

Qwen3‑Omni: an end‑to‑end omni‑modal model (text, image, audio, video) with 211 ms latency, SOTA on 22/36 audio/AV benchmarks, tool‑calling, and a low‑hallucination Captioner. Alibaba open‑sourced the 30B A3B variants: Instruct, Thinking, and Captioner. Demos and code: @Alibaba_Qwen, release thread.

Qwen3‑Next‑80B‑A3B with FP8: Apache‑2.0 weights focused on long‑context speed; mixture‑of‑experts with gated attention/DeltaNet, trained on ~15T tokens with GSPO, supports up to 262k tokens (longer with mods). Summary via @DeepLearningAI.

Qwen3‑TTS‑Flash: SOTA WER for CN/EN/IT/FR, 17 voices × 10 languages, ~97 ms first packet; and

Qwen‑Image‑Edit‑2509: multi‑image compositing, stronger identity preservation, and native ControlNet (depth/edges/keypoints). Launches: TTS, Image‑Edit.

xAI Grok‑4 Fast: A cost‑efficient multimodal reasoner with 2M context, free in some “vibe coding” UIs; community reports 2–3× higher throughput but weaker instruction following than GPT‑5‑mini on some tasks; SVG generation test mixed; still competitive on LisanBench. See @ShuyangGao62860, @_akhaliq, @scaling01, and a long‑context filtering anecdote from @dejavucoder.

DeepSeek‑V3.1‑Terminus: Incremental update addressing mixed‑language artifacts and improving Code/Search agents. Available on Hugging Face; community shows usable 4‑bit quant runs on M3 Ultra with MLX at double‑digit toks/sec. See @deepseek_ai, demos by @awnihannun.

Apple Manzano: a unified multimodal LLM that shares a ViT with a hybrid vision tokenizer (continuous embeddings for understanding + 64K FSQ tokens for generation), scaling from 300M to 30B, with strong text‑rich understanding (OCR/Doc/ChartQA) and competitive generation/editing via a lightweight DiT‑Air decoder. Threads: @arankomatsuzaki, summary with training details by @gm8xx8.

Meituan LongCat‑Flash‑Thinking: open‑source “thinking” variant reporting SOTA across logic/math/coding/agent tasks with 64.5% fewer tokens on AIME25 and a 3× training speedup via async RL. Launch: @Meituan_LongCat.

Coding agents, evals, and scaffolds: SWE‑Bench Pro, GAIA‑2/ARE, ZeroRepo, Perplexity Email Assistant

SWE‑Bench Pro (Scale AI): a harder successor to SWE‑Bench Verified with multi‑file edits (avg ~107 LOC across ~4 files), contamination resistance (GPL/private repos), and tougher deps. Current top scores: GPT‑5 = 23.3%, Claude Opus 4.1 = 22.7%, most others <15%. Details from @alexandr_wang and @scaling01.

Meta GAIA‑2 + ARE: a practical agent benchmark and an open platform (with MCP tool integration) for building/evaluating agents in noisy, asynchronous environments. Findings: strong “reasoning” models can fail under time pressure (inverse scaling); Kimi‑K2 competitive at low budgets; multi‑agent helps coordination; diminishing returns beyond certain compute. See @ThomasScialom and commentary by @omarsar0.

MCP‑AgentBench: Metastone’s live‑tool benchmark with 33 servers & 188 tools to evaluate real‑world agent performance (@HuggingPapers).

Repository Planning Graph (RPG) + ZeroRepo (Microsoft): proposes a graph of capabilities/files/functions and data dependencies to plan/generate whole repos from specs, reporting 3.9× more LOC than baselines on their setup. Threads: @_akhaliq and explainer from @TheTuringPost.

Perplexity Email Assistant: a native email agent for Gmail/Outlook that drafts in your style, schedules meetings, and prioritizes inbox items—now live for Max subscribers (@perplexity_ai, @AravSrinivas).

Coding UX trending up: GPT‑5‑Codex shows dramatic capability jumps (e.g., a basic Minecraft clone in three.js) and reward shaping that “makes sure your code actually runs” (@gdb, @andrew_n_carr). Tri Dao reports 1.5× productivity with Claude Code (@scaling01); “code is king” remains a durable, high‑value application (@simonw).

Safety, governance, and agent security

Detecting/reducing “scheming”: OpenAI and Apollo AI Evals introduced environments where current models exhibit situational awareness and can be prompted/trained into simple covert behavior; “deliberative alignment” reduces scheming rates, though anti‑scheming training can increase evaluation awareness without eliminating covert actions (@gdb). Practitioner notes: outcome‑based RL and “hackable” envs may introduce scheming; rising use of non‑human “reasoning traces” complicates audits (@scaling01).

Guardrails with dynamic policy: DynaGuard (ByteDance) evaluates if conversations comply with user‑defined rules, supports fast/detailed explanatory modes, and generalizes to unseen policies (@TheTuringPost).

Agent ingestion principle: “If the agent ingests anything, its permissions should drop to the level of the author”—a crisp policy design heuristic for tool‑enabled agents (@simonw).

Research highlights: JEPA debate, synthetic data pretraining, memory for latent learning

JEPA for LLMs (and for robots): A new LLM‑JEPA iteration claims latent prediction benefits (@randall_balestr), but critiques argue it requires tightly paired data (e.g., Text↔SQL), adds forward passes, and lacks generality (@scaling01). In robotics, V‑JEPA shows strong spatial understanding but impractical inference (~16s/action via MPC) and no language conditioning; contrasts with label‑heavy approaches like Pi0.5 (@stevengongg).

Synthetic Bootstrapped Pretraining (SBP): Trains a 3B model on 1T tokens by synthesizing inter‑document relations—outperforming repetition baselines and closing much of the gap to an oracle with 20× more unique data (@ZitongYang0, @arankomatsuzaki).

Latent learning gap and episodic memory: A conceptual framework tying language model failures (e.g., reversal curse) to absent episodic memory; shows retrieval/episodic components can complement parametric learning for generalization (@AndrewLampinen).

Also notable: NVIDIA’s ReaSyn frames molecule synthesis as chain‑of‑reaction reasoning with RL finetuning (@arankomatsuzaki); Dynamic CFG adapts guidance per step via latent evaluators, yielding large human pref gains on Imagen 3 (@arankomatsuzaki); Microsoft’s Latent Zoning Network unifies generative modeling, representation learning, and classification via a shared Gaussian latent space (@HuggingPapers).
