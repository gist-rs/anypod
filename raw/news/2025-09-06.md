China’s long‑context coding surge: Kimi K2‑0905 and Qwen3‑Max preview

Moonshot’s Kimi K2‑0905 (open weights) ships a practical agents upgrade: Kimi doubled context to 256k, improved coding and tool‑calling, and tuned integration with agent scaffolds (Cline, Claude Code, Roo). It’s already live on multiple stacks: Hugging Face weights/code, Together AI, vLLM deployment guide, LMSYS SGLang runtime (60–100+ TPS), Groq instant inference (200+ T/s, $1.50/M tokens), and Cline integration. Community reports emphasize that “agents really need ultra‑long context” for stability and tool orchestration (Teknium). Claims of “meets or beats Sonnet 4” surfaced in demos, while Kimi engineers acknowledged SWE‑Bench remains challenging (@andrew_n_carr, @bigeagle_xd).

Qwen3‑Max‑Preview (Instruct): 1T‑parameter scale, agent‑oriented behavior: Alibaba introduced its largest model yet (over 1T parameters), available via Qwen Chat, Alibaba Cloud API, and now OpenRouter (announcement, OpenRouter). Benchmarks and early users point to stronger conversations, instruction following, and agentic tasks relative to prior Qwen3 models. Community reaction frames it as a “US‑grade frontier model” with competitive pricing and throughput (reaction, scale tease). Details on dense vs MoE remain unspecified in public channels.

Evals, agents, and what to measure

“No evals” vs “evals that matter”: A widely‑shared thread argues many top code‑agent teams ship without formal evals, while vendors evangelize them; the nuance is that early 0→1 success often comes from dogfooding + error analysis before codifying evals (@swyx, receipts). Follow‑ons advocate for richer, causal evals of long‑horizon capability (e.g., months‑long tasks, protocol replication, strategy games, real‑world setups) and domain‑specific enterprise workflows that today’s leaderboards miss (@willdepue, ideas, @levie, @BEBischof). A pragmatic tip: use models as discriminators to rank outputs—generator/discriminator gaps can be leveraged in practice (@karpathy).

Operationalizing evals and traces in agent stacks: CLI‑first agents plus semantic search can outperform ad‑hoc RAG for document tasks; LlamaIndex shows SemTools handling 1,000 arXiv papers with UNIX tooling + fuzzy semantic search (post). For RL pipelines, THUDM’s slime provides a clean rollout abstraction integrating tool calls and state transitions, reducing glue code in agentic RL experiments (overview).

Inference and post‑training advances

Decoding and planning: Meta’s Set Block Decoding (SBD) samples multiple future tokens in parallel, cutting forward passes 3–5× with no architecture changes and KV‑cache compatibility; trained models match standard NTP performance on next‑token prediction (summary). For agents, “always reasoning” (ReAct) isn’t optimal—new work trains models to learn when to plan, dynamically allocating test‑time compute to balance cost and performance (thread, paper context).

Post‑training theory and results: “RL’s Razor” argues on‑policy RL forgets less than SFT—even at matched accuracy—by biasing toward KL‑minimal solutions, with toy + LLM experiments supporting reduced catastrophic forgetting (summary). A “Unified View of LLM Post‑Training” shows SFT and RL optimize the same reward‑with‑KL objective; Hybrid Post‑Training (HPT) switches between them via simple performance feedback and consistently beats strong baselines across scales/families (overview). On the empirical side, Microsoft’s rStar2‑Agent‑14B uses agentic RL to reach frontier‑level math (AIME24 80.6, AIME25 69.8) in just 510 RL steps, with shorter, more verifiable chains of thought (results).

GPU stacks, kernels, and platforms

ROCm quality regression in PyTorch: Analysis alleges a growing deficit of ROCm‑only skipped/disabled tests (>200 each), with a net increase since June 2025; reports say even core transformer ops (e.g., attention) have been disabled for months, harming developer trust. AMD leadership has reportedly reprioritized fixes (report). PyTorch maintainers note broad test‑skipping is endemic and requires sustained contributor attention across subsystems (context, quip). Separately, PyTorch published a kernel deep‑dive on 2‑simplicial attention implemented in TLX (Triton low‑level extensions) (kernel post).

Infra momentum and meetups: Together AI announced a $150M Series D led by BOND (Jay Simons to board) to scale inference infra (annc); Baseten also raised $150M Series D as it rolls out performance work and EmbeddingGemma support (annc). vLLM is hosting a Toronto meetup on distributed inference, spec decode, and FlashInfer (event) and already supports Kimi K2 deployments (support).

OpenAI ecosystem: ChatGPT branching, Responses API, and Codex

Product/API shifts: ChatGPT now supports conversation branching (@gdb; @sama). OpenAI’s Responses API got an in‑depth explainer (thread); the AI SDK v5 now defaults the OpenAI provider to Responses (Completions remains available) (note). Some devs countered that Responses complicates context portability and stateless usage in practice (critique), while others observed improved “chain‑of‑thought preservation” in ongoing conversations vs Chat Completions (anecdote).

Coding agents and GPT‑5 Pro: Multiple practitioners report GPT‑5 Pro inside Codex can unblock gnarly engineering problems with deeper, slower passes; “smarter” beats “faster” was the sentiment in a public exchange with Sam Altman (experience, follow‑up, @sama). The Codex CLI/IDE continues shipping rapidly (changelog).

Embeddings and retrieval move on‑device (and hit limits)

Small, fast, local: Google’s new open‑source EmbeddingGemma got day‑0 platform support (e.g., Baseten), with reports of embedding 1.4M docs in ~80 minutes on an M2 Max for free and better quality than older large paid models (Baseten, field result). On‑device retrieval is getting easier: SQLite‑vec + EmbeddingGemma runs fully offline across languages/runtimes (guide).

Single‑vector limits: New theory/benchmark “LIMIT” shows hard lower bounds on top‑k retrieval under fixed embedding dimensions, with SOTA models failing on deliberately stress‑tested simple tasks—evidence that some combinations of relevant documents are intrinsically unrecoverable with single‑vector embeddings, motivating multi‑vector/late‑interaction approaches (summary).
