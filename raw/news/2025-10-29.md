The good news is that Sama and team have landed the plane successfully: with tens of billions of dollars at stake, both the for-profit and Microsoft renegotiations have concluded and there is a clean cap table and corporate structure now (credit Amir Efrati), clearing the way for a "likely" OpenAI IPO:



A stacked bar chart showing the approximate ownership stakes for OpenAI's shareholders in its new public benefit corporation restructuring.


Microsoft let go of their exclusivity in exchange for a $250b OpenAI commit to Azure spend, and now OpenAI is free to work with other vendors, while Satya is now saying "I would love to have Anthropic... If Google wants to put Gemini on Azure, please do.”

The other large financial number announced in the livestream is that this year's 30GW worth of compute deals have totaled $1.4T ($47B per GW), and that the aspirational goal is for OpenAI to eventually build 1GW a week at $20B per GW (meaning about $1T a year of compute capex). Given stated goals of reaching 125GW, this means OpenAI will be wrangling about 3-4 trillion dollars worth of infra by 2033, about half the initially speculated 7 trillion number.

No, you're not alone in thinking this is crazy, all of this is entirely unprecedented and yet possible, perhaps probable.

Perhaps for an AI Engineer audience, the more material announcements are in the platform "pivot" that OpenAI seems to have announced: a decreased emphasis on first party apps (odd given that they have a CEO of Apps):

A diagram showing the various components and infrastructure of OpenAI's ecosystem, including ChatGPT, Sora, devices, models, chips
and now more strongly than ever emphasizing the platform approach, even citing the Bill Gates Line:

A person sitting at a desk with a laptop, presenting a slide about platform value being created more by people building on the platform than by the platform builder


If you watch OpenAI closely, this is all the signal you need.



AI Twitter Recap
OpenAI’s new structure, Microsoft deal, and “open weights”

OpenAI announced a recapitalization and reorg: the non-profit is now the OpenAI Foundation, the for‑profit becomes a Public Benefit Corporation (PBC). The Foundation holds special voting rights to appoint/replace the PBC board, owns equity valued at ~$130B, and holds a warrant that grants additional equity if the share price >10× in 15 years. OpenAI framed this as keeping the non‑profit “in control” while resourcing the mission (OpenAI, @stalkermustang highlights). Sam Altman and Jakub previewed priorities and took questions in a live session (@OpenAI, @sama).

Analysts summarized the Microsoft agreement: Microsoft now holds ~27% on a diluted basis; remains OpenAI’s frontier model partner with Azure API exclusivity until an AGI declaration verified by an independent panel; IP rights through 2032 (including post‑AGI with safety guardrails); OpenAI commits to ~$250B in additional Azure purchases; Microsoft loses right of first refusal on compute; OpenAI may co‑develop with third parties and provide APIs to US national security customers on any cloud; API products remain Azure‑exclusive (@koltregaskes).

“OpenAI is now able to release open‑weight models that meet requisite capability criteria,” per OpenAI’s policy language—this drew immediate attention from practitioners tracking the open ecosystem (@reach_vb). Observers circulated provisional equity splits of Foundation ~26%, Microsoft ~27%, employees/investors ~47% (@scaling01), though caution is warranted pending formal filings.

Key open governance and safety reads: questions on Foundation control, mission vs. commercial goals, and AGI definitions under the Microsoft agreement (@robertwiblin). AGI timelines on Metaculus have lengthened by ~3 years since February, now May 2033 for “first AGI” and Oct 2027 for a weak, non‑robotic standard (@robertwiblin).

Agents go first‑class: GitHub Universe, LangChain Deep Agents, and API design for agents

GitHub Agent HQ and VS Code Agent Sessions: GitHub announced Agent HQ to orchestrate “any agent, any time, anywhere,” with native collaborators (e.g., Claude, Devin) integrated into GitHub workflows. VS Code Insiders now ships an Agent Sessions view with OpenAI Codex and Copilot CLI, a built‑in plan agent, isolated sub‑agents, and a Copilot Metrics dashboard to track impact across any coding agent. Multiple Codex instances can run in parallel to complete tasks and open PRs (@github, @code, @burkeholland, @pierceboggan, @mikeyk, @cognition).

LangChain Deep Agents 0.2: Introduces a “backend” abstraction to swap the agent filesystem for a local FS, DB, or remote VM; focuses on long‑running, high‑performance agents with context compression, file‑system offloading, and subagent isolation. Positioning: a general‑purpose harness for building systems like Deep Research or coding agents (@hwchase17, @LangChainAI, context engineering summary).

API design for agents: Postman’s “AI‑ready APIs” argues most agents fail on weak machine‑readable documentation; it pushes predictable structures, standardized behavior, synced schema, and auto‑generated, contextual docs (Agent Mode) to reduce guesswork ( @_avichawla).

Educational resources: DeepLearning.AI and AMD launched an “Intro to Post‑Training” course covering SFT, RLHF, PPO/GRPO, LoRA, evals/red‑teaming, and production pipelines, with AMD GPUs backing fine‑tuning/RL runs (@AndrewYNg, @realSharonZhou).

Serving, observability, and infra

vLLM Sleep Mode: zero‑reload model switching for multi‑model serving with 18–200× faster switches and 61–88% faster first token vs cold starts. Two levels: L1 offloads weights to CPU; L2 discards weights; preserves allocators, CUDA graphs, JIT kernels across sleeps; works with TP/PP/EP (@vllm_project).

Tool‑calling reliability with Kimi K2 on vLLM: After fixing add_generation_prompt, empty content handling, and stricter tool‑call ID parsing, K2 achieved >99.9% request success and 76% schema accuracy (4.4× improvement). An “Enforcer” to constrain tool generation is coming. The K2 vendor verifier now reports trigger similarity and schema accuracy case‑by‑case (vLLM deep dive, @Kimi_Moonshot, vendor tips).

Observability: Red Hat details token‑level metrics for LLM systems—TTFT, TPOT, cache hit ratios, and end‑to‑end traces from ingress to vLLM workers—enabling cache‑aware, routing‑aware monitoring on OpenShift AI 3.0 (@RedHat_AI).

Communication for MoE on cloud: UCCL‑EP is a GPU‑driven expert‑parallel library targeting public clouds (e.g., AWS EFA) and heterogeneous GPUs/NICs, API‑compatible with DeepEP, addressing slow MoE comms reported with EFA+perplexity kernels (@ziming_mao).

“Train on your laptop” claims: Tinker added gpt‑oss and DeepSeek model families, marketing the ability to train a 671B MoE locally “in a few lines” without CUDA/cluster setup. Treat this as an abstraction stack amortizing shared infra across users rather than literal local pretraining (@thinkymachines, @dchaplot, skeptic’s framing).

New models and retrieval systems

Late‑interaction retrieval: Liquid AI released LFM2‑ColBERT‑350M, a 350M multilingual late‑interaction retriever with token‑level precision, precomputed doc embeddings, and strong cross‑lingual performance. Claims include best cross‑lingual under 500M, >1K docs/sec encoding, and inference speed on par with smaller ModernColBERT variants (@LiquidAI_, @maximelabonne, ColBERT community reaction).

IBM Granite 4 Nano (Apache‑2.0): New small models; the 1B variant reportedly outperforms Qwen3‑1.7B across math/coding and more (@mervenoyann, HF blog).

NVIDIA Nemotron Nano 2 VL (open): A 12B VLM for document/video understanding (4 images or 1 video per prompt), hosted across platforms (Replicate, Baseten, Nebius) and accompanied by an 8M‑sample CC‑BY‑4.0 dataset for OCR/multilingual QA/reasoning. NVIDIA emphasized broader support for openly developed AI and contributed 650+ models/250 datasets on HF (dataset thread, Replicate, Baseten, Nebius, NVIDIA).

MiniMax M2 (open weights): Strong agentic/coding performance, architecture akin to Qwen3 with full attention, per‑head per‑layer QK‑Norm, optional sliding‑window attention disabled by default, and 10B active expert MoE sparsity vs Qwen3’s 22B. Available via OpenRouter/Roo Code/Ollama Cloud; note integration pitfalls like stripping <think> segments can degrade tool‑use (architecture analysis, OpenRouter, Ollama, integration gotcha).

Open science in bio/robotics: OpenFold3 launched as an open foundation model for 3D structures of proteins/nucleic acids/small molecules (@cgeorgiaw). LeRobot v0.4 ships a streamable dataset format, LIBERO/Meta‑World sim support, data processors, multi‑GPU training, hardware plugins, and SOTA policies (PI0/PI0.5, Gr00t N1.5) plus an open course (@LeRobotHF).

Realtime voice and multimodal assistants

Cartesia Sonic‑3 (SSM, not Transformers): $100M Series C and a real‑time voice model with 90ms model latency (190ms end‑to‑end), 42 languages, natural emotional range/laughter. Built on state‑space models pioneered by S4/Mamba work; widely praised by sequence‑modeling researchers (launch, @tri_dao).

Google Gemini for Home (early access, U.S.): A voice assistant blending classic “Hey Google” requests with Gemini Live conversational sessions on speakers/displays (@Google).

Veo 3.1: Google’s filmmaking tool update emphasizes richer audio, narrative control, and realism (@dl_weekly).

Safety, governance, and scaling research

Anthropic’s Responsible Scaling Policy in practice: A detailed Opus 4 sabotage risk report was published alongside an external review from METR, with improved transparency around redactions. Reviewers agreed with the risk assessment and called for broader third‑party scrutiny across diverse threat models (Anthropic, METR).

Decentralized training feasibility: Epoch AI argues 10 GW training runs across ~two dozen geographically distributed sites linked by long‑haul networks are technically feasible, citing Microsoft’s planned multi‑GW Fairwater datacenter as evidence of distributed AI training architectures on the horizon (@EpochAIResearch).

Multilingual scaling laws: ATLAS (774 experiments, 10M–8B params, 400+ languages) provides compute‑optimal crossover points for pretrain‑from‑scratch vs finetune and quantifies cross‑lingual transfer (e.g., which languages help/hurt English at 2B scale). Useful for data‑constrained LLM scaling beyond English (@ShayneRedford, @Muennighoff).

Distillation for post‑training: On‑policy distillation emerged as a practical recipe to post‑train smaller LLMs with dense, on‑policy feedback; Qwen reports strong math‑reasoning gains and continual‑learning recovery in experiments (@Alibaba_Qwen, community implementers).

---

4 months after MiniMax M1, Hailuo AI is back with MiniMax M2 (free chatbot, weights, github, docs) with some impressive, but measured claims: a very high 23x sparsity (Qwen-Next still beats it) and SOTA-for-Open-Source performance:

Bar graph showing the Artificial Analysis Intelligence Index v3.0 with various AI models and their performance scores, with MiniMax M2


There are some hairs - it is a very verbose model and there was no tech report this time, but overall this is a very impressive model launch that comes clsoe to the frontier closed models under a very comprehensive set of benchmarks.

Bar graph showing performance benchmarks of various AI models across different tasks, with MiniMax M2 highlighted in red and compared against other models like






AI Twitter Recap
MiniMax M2 open-weights release: sparse MoE for coding/agents, strong evals, and architecture clarifications

MiniMax M2 (open weights, MIT): MiniMax released M2, a sparse MoE model reported as ≈200–230B total with 10B active parameters, positioned as “Agent & Code Native.” The model is temporarily free via API, priced at “8% of Claude Sonnet” and ~2x faster per MiniMax, and licensed MIT. It’s day-0 supported in vLLM and generally available on Hugging Face, ModelScope, OpenRouter, Baseten, Cline, and more. See announcement and availability: @MiniMax__AI, @vllm_project, @reach_vb, @ArtificialAnlys, @MiniMax__AI, @QuixiAI, @basetenco, @cline, @_akhaliq.

Benchmarks and cost profile: On the Artificial Analysis index, M2 hits the new “all-time high” for open weights and #5 overall; strengths include tool-use and instruction following (e.g., Tau2, IFBench), with potential underperformance vs. DeepSeek V3.2/Qwen3-235B on some generalist tasks. Reported API pricing of $0.3/$1.2 per 1M input/output tokens, but high verbosity (≈120M tokens used in their eval) can offset sticker price. Fits on 4×H100 in FP8. Details and per-benchmark scores: @ArtificialAnlys, @ArtificialAnlys.

Architecture notes (correcting speculation): Early readings inferred GPT-OSS-like FullAttn+SWA hybrid; an M2 engineer clarified the released model is full attention. SWA and “lightning/linear” variants were tried during pretrain but were dropped due to degraded multi-hop reasoning (they also tried attention-sink). Public configs/code indicate use of QK-Norm, GQA, partial RoPE (and variants), and MoE choices like no shared expert; community observed “sigmoid routing” and “MTP.” Threads and clarifications: @Grad62304977, @eliebakouch, @yifan_zhang_, @zpysky1125, @eliebakouch.

Ecosystem PRs and tooling: Day-0 inference PRs landed in vLLM and sglang; more deploy paths emerging (anycoder demos, ModelScope, Baseten library). PRs and threads: @vllm_project, @eliebakouch, @eliebakouch, @_akhaliq.

Post-training and reasoning: on-policy distillation momentum, long-horizon stress-tests, and agent frameworks

On-Policy Distillation (OPD) resurges: A comprehensive writeup shows OPD—training the student on its own rollouts with teacher logprobs as dense supervisory signal—can match or beat RL for significantly less compute (claims of “1800 hours OPD vs 18,000 hours RL” in one setup) for math reasoning and internal chat assistants, with wins on AIME-style tasks and chat quality. The method reduces OOD shock vs. SFT-only and resembles DAGGER in spirit. Endorsements from DeepMind/Google researchers and TRL support underscore that Gemma 2/3 and Qwen3-Thinking use variants of this. Read and discussion: @thinkymachines, @lilianweng, @_lewtun, @agarwl_, @barret_zoph.

RL coding results nuance: Multiple reports reiterate that RL often boosts pass@1 but not pass@{32,64,128,256} in code benchmarks—evidence of mode/entropy collapse—across PPO/GRPO/DAPO/REINFORCE++. Threads: @nrehiew_, @nrehiew_.

Long-horizon reasoning (R-HORIZON): New benchmark composes interdependent chains across math/code/agent tasks; state-of-the-art “thinking” models degrade sharply as horizon grows (e.g., DeepSeek-R1: 87.3% → 24.6% at 5 linked problems; R1-Qwen-7B: 93.6% → 0% at 16). RLVR+GRPO training on such chains improves AIME24 by +17.4 (n=2) and single-problem by +7.5. Data and train sets are on HF. Overview: @gm8xx8.

Recursive LMs and long context: “Recursive LM” composes a root LM with an environment LM that accumulates evolving context/prompt traces; shows strong performance on the long-context OOLONG benchmark. Call for task ideas: @ADarmouni, @lateinteraction.

Architectures and attention design: shifting away from linear attention, MoE insights, and context compression

Linear/SWA vs. full attention trade-offs: Multiple practitioners observed teams abandoning “naive linear attention” and SWA hybrids in favor of full attention after ablations showed reasoning regressions at scale—even when hybrids helped throughput/long-context earlier (cf. GPT-OSS, Minimax M1 ablations). Minimax confirms SWA experiments hurt multi-hop reasoning in M2. Threads: @Grad62304977, @eliebakouch, @zpysky1125.

Qwen3 MoE and expert attention: Community deep dives analyze Qwen3’s depth-wise upcycling and MoE internals, with calls to “always visualize” to catch emergent patterns. “Expert Attention” and routing details surfaced in related papers. Threads and visuals: @ArmenAgha, @AkshatS07, @eliebakouch.

Glyph: visual-text compression for long context: Zhipu AI’s Glyph renders long text into images and uses VLMs to process them, achieving 3–4× token compression without performance loss in reported tests—turning long-context into a multimodal efficiency problem. Paper/code/weights: @Zai_org, @Zai_org, @Zai_org.

Infra and performance: collectives at 100k+ GPUs, FP8 that actually wins end-to-end, and real-world hardware notes

Meta’s NCCLX for 100k+ GPUs: New paper/code for large-scale collectives aimed at 100k+ GPU clusters, released under the Meta PyTorch umbrella. Paper + repo: @StasBekman.

FP8 training, done right: Detailed Zhihu write-up shows substantial end-to-end wins from fused FP8 operators and hybrid-linear design: up to 5× faster kernels vs. TransformerEngine baselines on H800, and +77% throughput in a 32×H800 large-scale run (with memory reductions and stable loss). Key fusions: Quant+LN/SiLU+Linear, CrossEntropy reuse, fused LinearAttention sub-ops, MoE routing optimizations. Summary and links: @ZhihuFrontier.

DGX Spark concerns: Early reports suggest DGX Spark boards are drawing ~100W vs. a 240W rating and achieving roughly half the expected performance, with heat and stability issues observed. Query if devices were de-rated before launch: @ID_AA_Carmack.

vLLM updates: Beyond day-0 M2 support, vLLM released a “Semantic Router” update with Parallel LoRA execution, lock-free concurrency, and FlashAttention 2 for 3–4× faster inference; Rust×Go FFI for cloud-native deploys. Release: @vllm_project.

Frameworks, libraries, and courses

LangChain/Graph v1 and “agent harnesses”: LangChain v1 adds standard content blocks to unify providers, a create_agent abstraction, and a clarified stack: LangGraph (runtime), LangChain (framework), DeepAgents (harness). New free courses (Python/TS) cover agents, memory, tools, middleware, and context engineering patterns. Announcements and guides: @LangChainAI, @bromann, @sydneyrunkle, @hwchase17, @hwchase17, @_philschmid.

Hugging Face Hub v1.0 and streaming backend: Major backend overhaul enabling “train SOTA without storage” via large-scale dataset streaming; new CLI and infra modernization. Threads: @hanouticelina, @andimarafioti.

Keras 3.12: Adds GPTQ quantization API, a model distillation API, PyGrain datasets across the data API, plus new low-level ops and perf fixes. Release notes: @fchollet, @fchollet.

Safety, enterprise, and benchmarking

Anthropic enterprise traction and finance vertical: A survey suggests Anthropic overtook OpenAI in enterprise LLM API share; Anthropic also launched “Claude for Financial Services” with a Excel add-in, real-time market connectors (LSE, Moody’s, etc.), and prebuilt Agent Skills (cashflows, coverage reports). Announcements: @StefanFSchubert, @AnthropicAI, @AnthropicAI.

OpenAI model behavior & mental health: OpenAI updated the Model Spec (well-being, real-world connection, complex instruction handling) and reported improved handling of sensitive mental health conversations after consulting 170+ clinicians, with claimed 65–80% reduction in failure cases; GPT-5 “safety progress” noted. Updates: @OpenAI, @w01fe, @fidjissimo.

New capability tracking: Epoch released the Epoch Capabilities Index (ECI) to track progress across saturated benchmarks via a transparent, open methodology. Launch: @EpochAIResearch.
