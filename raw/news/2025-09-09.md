Coding Agents and Tooling Momentum

Cognition raises $400M to scale Devin: Cognition announced a $400M round at a $10.2B post-money valuation to “advance the frontier of AI coding agents,” led by Founders Fund with Lux, 8VC, Neo and others participating. The team highlighted customer expansion and the Windsurf team joining, and is hiring across product, infra, and post‑training (announcement 1, 2, team note, plans clip). Commentary: @swyx is joining Cognition, laying out why he’s “buying” the agent-lab thesis and how positioning across sync/async workflows matters for dominance in the “Decade of Agents” (thread).

Agent dev stacks getting simpler and more capable:

Vercel shipped an OSS “vibe coding platform” built on the Vercel AI SDK, Gateway, Sandbox, and a tuned GPT‑5 agent loop (tool use: file IO, commands, package install, autofix) with a one‑shot demo coding a multiplayer Pong game in Go (demo).

Claude Code’s loop is intentionally minimal: a single master loop + async buffer, direct tools, and TODO-based planning; simplicity beats swarm orchestration for debuggability and reliability (analysis).

Coding evals: Kimi K2‑0905 on Groq hit 94% and ranked 7th on Roo Code, becoming the first open-weight model to break 90+ while also being the fastest/cheapest in the top 10 (leaderboard). Tim Dettmers reports the practical frontier for coding assistants feels increasingly open-weight: GLM‑4.5 is “$3/month” and ~Sonnet quality; Kimi K2.1 Turbo ~3× faster and ~7× cheaper vs Opus 4.1, with GPT‑5 excelling mainly on complex spec work (take).

Model and Inference Advances

Kimi K2 0905 and Qwen3-ASR:

Kimi K2 0905 (1T params, architecture unchanged) boosts agentic capabilities: Terminal‑Bench Hard from 14→23% and Tau2‑Bench Telecom 61→73%; context doubled from 128k→256k. Intelligence +2 on Artificial Analysis’ AAII; now serving on Kimi’s site (summary, live note).

Alibaba’s Qwen3‑ASR released a single model for multilingual transcription (EN/CN + 9 languages), autodetect, robust to BGM/noise/rap, with <8% WER and custom contextual biasing. Demos on ModelScope/HF; API available (launch).

Faster decoding and lighter KV:

Meta’s Set Block Decoding (SBD) enables 3–5× decoding speedups on existing LMs without architectural changes, matching NTP performance and preserving exact KV cache—parallel generation via masked/discrete diffusion formulation (overview, details).

KV cache and quant innovation: AutoRound is now in SGLang (PR), Turing Post surveyed KV compression (quantization, low‑rank, Slim Attention, XQuant) with tradeoffs (thread), and QuTLASS v0.1.0 brings 4‑bit NVFP4 microscaling and fast transforms to Blackwell GPUs (release). AlgoPerf v0.6 adds a rolling leaderboard, JAX jit, and lower compute costs for algorithmic benchmarking (update); ZeroGPU AOT compilation internals for PyTorch were documented by HF (blog).

Multimodal Generation, Video, and “Vibe Coding”

Veo 3 goes GA and cheaper: Google’s Veo 3 and Veo 3 Fast are now GA in the Gemini API with ~50% price cuts ($0.40/s and $0.15/s), 1080p output, and 9:16 vertical video support—positioned for scaled production (dev blog, pricing breakdown, PM note).

Community workflows and tooling:

“Nano Banana” (Gemini 2.5 Flash Image Preview) catalyzed a weekend of “vibe‑coded” projects—now open-sourced for remix in Google AI Studio; teams report 1‑click reuse and playful gotchas (e.g., always rendering clocks at 10:10) (open-source pack, quirk).

Qwen’s “paper → website” flow turns a research paper into a deployable site in minutes (demo). Lmarena added multi‑turn image editing evals so the community can compare iterative refinement across models (incl. “nano banana”) (feature). For doc RAG UX, ColQwen2 + Weaviate powers token‑wise similarity maps for visual PDF search and patch highlighting (build).

Agents, Post-Training RL, and Evaluation Practice

Towards iterated self‑improvement: FAIR’s Exploratory Iteration (ExIt) trains LLMs for inference‑time self‑improvement via an automatic curriculum that bootstraps from the model’s own prior responses, prioritizing partial histories with high return variance in GRPO groups. ExIt outperforms GRPO on contest math, BFCLv3 multi‑turn tasks, and MLE‑bench (+22%) while training only single‑step improvements (thread).

Online vs offline RL and evals:

Evidence continues to show a performance gap favoring online RL (PPO/GRPO) over offline methods like DPO at scale, though semi‑online iterations (on‑policy sampling + negative gradients) narrow the gap; data quality still dominates algorithm choice (summary).

Why many “agents” underdeliver: decision‑making has near‑zero error tolerance and sparse data vs generative tasks; most failures are coarse task scoping and unstructured environments rather than LLM shortcomings (debate recap).

RAG evals moving from “dead” unit tests to “living” loops: RAGGY (open‑source REPL) enables what‑if iteration for RAG, and there’s a strong push to integrate pre‑prod tests with production observability and human review rather than treating them as separate silos (RAGGY, evals take). Also see practical “Agentic RAG” architectures leveraging tool use and multi‑step reasoning (guide).

Robotics and Embodied AI

Multi‑robot planning via RL: Google DeepMind’s RoboBallet (with Intrinsic and UCL) choreographs up to 8 robot arms for collision‑free task and motion planning, outperforming traditional methods by ~25%, and generalizing to new workflows in seconds via RL‑learned coordination principles (announcement, more).

Open hardware stacks and dexterous manipulation: Pollen Robotics outfitted Reachy 2 with dual open‑source “Amazing Hand” grippers for fine manipulation; native integration coming (demo). X Square announced WALL‑OSS (open base model) and the Quanta X2 robot with auto‑mop and dexterous hand; Alibaba Cloud led a $140M A+ round (>$280M raised in <2 years) (summary). OpenPI’s pi‑05 is now in openpi with PyTorch support (release).

Benchmarks, Leaderboards, and Enterprise

Text leaderboards move: lmarena added two new entries into its Top 10 Text leaderboard: Qwen3‑max‑preview (#6, proprietary) and Kimi‑K2‑0905‑preview (#8, modified MIT), putting Kimi in contention for top open‑weight alongside Qwen and DeepSeek variants (update, model link). Artificial Analysis' K2‑0905 measurements mirror improved agentic performance (details).

Gov and enterprise:

Perplexity launched “Perplexity for Government”: secure by default, zero data usage, premium model access, and no enterprise contracts; also brought Perplexity Finance to iOS/Android (launch, follow‑up, finance mobile).

Anthropic endorsed California SB 53 (Sen. Scott Wiener), a transparency‑focused state framework for governing frontier AI in lieu of a federal standard (statement, context).
