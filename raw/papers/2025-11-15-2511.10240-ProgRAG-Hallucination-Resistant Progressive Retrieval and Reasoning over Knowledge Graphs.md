Abstract
Large Language Models (LLMs) demonstrate strong reasoning capabilities but struggle with hallucinations and limited
transparency. Recently, KG-enhanced LLMs that integrate
knowledge graphs (KGs) have been shown to improve reasoning performance, particularly for complex, knowledgeintensive tasks. However, these methods still face significant
challenges, including inaccurate retrieval and reasoning failures, often exacerbated by long input contexts that obscure
relevant information or by context constructions that struggle
to capture the richer logical directions required by different
question types. Furthermore, many of these approaches rely
on LLMs to directly retrieve evidence from KGs, and to selfassess the sufficiency of this evidence, which often results in
premature or incorrect reasoning. To address the retrieval and
reasoning failures, we propose ProgRAG, a multi-hop knowledge graph question answering (KGQA) framework that decomposes complex questions into sub-questions, and progressively extends partial reasoning paths by answering each subquestion. At each step, external retrievers gather candidate evidence, which is then refined through uncertainty-aware pruning by the LLM. Finally, the context for LLM reasoning is optimized by organizing and rearranging the partial reasoning
paths obtained from the sub-question answers. Experiments
on three well-known datasets demonstrate that ProgRAG outperforms existing baselines in multi-hop KGQA, offering improved reliability and reasoning quality.

Conclusion
In this paper, we introduce ProgRAG, a novel progressive retrieval and reasoning framework for multi-hop KGQA. ProgRAG decomposes a complex question into sub-questions,
and incrementally constructs reasoning paths by iteratively
retrieving candidate evidence with external retrievers and refining it through LLM-based pruning with uncertainty estimation. This progressive strategy enables more accurate
retrieval and reasoning by dynamically optimizing the input context and mitigating hallucinations. Extensive experiments on benchmark datasets demonstrate that ProgRAG
consistently outperforms state-of-the-art baselines, achieving improved reasoning accuracy and reliability.
