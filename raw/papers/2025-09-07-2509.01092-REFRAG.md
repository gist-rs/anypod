บทความนี้นำเสนอ REFRAG ซึ่งเป็น กรอบการถอดรหัส (decoding framework) ที่มีประสิทธิภาพสูง สำหรับการประยุกต์ใช้ Retrieval-Augmented Generation (RAG) เพื่อแก้ไขปัญหาความล่าช้าและหน่วยความจำ ที่เกิดจากการประมวลผลอินพุตแบบบริบทที่ยาวมากใน Large Language Models (LLMs) REFRAG ทำได้โดยการ บีบอัดส่วนของบริบท (compress context chunks) ที่ไม่จำเป็นมากนักให้เป็น embeddings ที่เล็กกว่า และใช้ นโยบายการเรียนรู้แบบเสริมกำลัง (Reinforcement Learning policy) เพื่อตัดสินใจเลือกส่วนที่สำคัญที่สุดที่ควรเก็บเป็นโทเค็นดั้งเดิม วิธีการนี้ช่วย เร่งความเร็ว Time-to-First-Token (TTFT) ได้อย่างมากถึง 30.85 เท่า และ ขยายขนาดบริบทของ LLMs ได้ถึง 16 เท่า โดยไม่ลดทอนประสิทธิภาพหรือความแม่นยำ ซึ่งมีความสำคัญอย่างยิ่งต่อแอปพลิเคชันที่ต้องการความรวดเร็ว เช่น การสนทนาแบบหลายเทิร์นและการสรุปเอกสารขนาดยาว
