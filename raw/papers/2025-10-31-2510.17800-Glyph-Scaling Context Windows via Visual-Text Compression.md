Glyph: Scaling Context Windows via Visual-Text Compression
Jiale Cheng1,2* , Yusen Liu2* , Xinyu Zhang2* , Yulin Fei2* , Wenyi Hong2,3
Ruiliang Lyu2 , Weihan Wang2 , Zhe Su2 , Xiaotao Gu2 , Xiao Liu2,3 , Yushi Bai2,3 Jie Tang3, Hongning Wang1 , Minlie Huang1†
1The Conversational Artificial Intelligence (CoAI) Group, Tsinghua University 2Zhipu AI
3The Knowledge Engineering Group (KEG), Tsinghua University chengjl23@mails.tsinghua.edu.cn, aihuang@tsinghua.edu.cn
Abstract
Large language models (LLMs) increasingly rely on long-context modeling for tasks such as document understanding, code analysis, and multi-step reasoning. However, scaling con-text windows to the million-token level brings prohibitive computational and memory costs, limiting the practicality of long-context LLMs. In this work, we take a different perspec-tive—visual context scaling—to tackle this challenge. Instead of extending token-based se-quences, we propose Glyph, a framework that renders long texts into images and processes them with vision–language models (VLMs). This approach substantially compresses textual input while preserving semantic information, and we further design an LLM-driven genetic search to identify optimal visual rendering con-figurations for balancing accuracy and com-pression. Through extensive experiments, we demonstrate that our method achieves 3–4× to-ken compression while maintaining accuracy comparable to leading LLMs such as Qwen3-8B on various long-context benchmarks. This compression also leads to around 4× faster prefilling and decoding, and approximately 2× faster SFT training. Furthermore, under extreme compression, a 128K-context VLM could scale to handle 1M-token-level text tasks. In addition, the rendered text data benefits real-world multimodal tasks, such as document un-derstanding. Our code and model are released at https://github.com/thu-coai/Glyph.
