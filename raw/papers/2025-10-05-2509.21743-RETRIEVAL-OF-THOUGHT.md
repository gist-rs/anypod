RETRIEVAL-OF-THOUGHT: EFFICIENT REASONING
VIA REUSING THOUGHTS
Ammar Ahmed1*, Azal Ahmad Khan1*, Ayaan Ahmad2
, Sheng Di3
, Zirui Liu1
, Ali Anwar1
1University of Minnesota, 2University of California Santa Cruz, 3Argonne National Laboratory
{ahme0599, khan1069, zrliu, aanwar}@umn.edu
ayahmad@ucsc.edu, sdi1@anl.gov
ABSTRACT
Large reasoning models improve accuracy by producing long reasoning traces,
but this inflates latency and cost, motivating inference-time efficiency. We propose Retrieval-of-Thought (RoT), which reuses prior reasoning as composable
â€œthought" steps to guide new problems. RoT organizes steps into a thought graph
with sequential and semantic edges to enable fast retrieval and flexible recombination. At inference, RoT retrieves query-relevant nodes and applies rewardguided traversal to assemble a problem-specific template that guides generation.
This dynamic template reuse reduces redundant exploration and, therefore, reduces output tokens while preserving accuracy. We evaluate RoT on reasoning
benchmarks with multiple models, measuring accuracy, token usage, latency, and
memory overhead. Findings show small prompt growth but substantial efficiency
gains, with RoT reducing output tokens by up to 40%, inference latency by 82%,
and cost by 59% while maintaining accuracy. RoT establishes a scalable paradigm
for efficient LRM reasoning via dynamic template construction through retrieval
